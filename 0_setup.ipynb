{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipelines Sample\n",
    "\n",
    "## 離反予測を用いた SageMaker Pipelines の ML パイプライン構築\n",
    "\n",
    "### シナリオ\n",
    "\n",
    "電話回線の離反データセット（回線ごとのデータと離反した/しなかったの結果が残る）を使って、\n",
    "SageMaker Pipelines を用いたML パイプラインを構築します。\n",
    "データの詳細については[こちら](https://github.com/aws-samples/amazon-sagemaker-examples-jp/blob/master/xgboost_customer_churn/xgboost_customer_churn.ipynb)に詳細があります。  \n",
    "\n",
    "3333 行の 元データを 1111 行ずつ 3 分割し、それぞれ 1 日目に入手するデータ、 2 日目に入手するデータ、 3 日目に入手するデータと仮定する。  \n",
    "\n",
    "#### 1日目\n",
    "\n",
    "まずは 1 日目のデータを使ってモデルを開発、学習し、モデルを評価し、デプロイすることを、手で（SageMaker Pipelines を使わない）で行います。\n",
    "\n",
    "#### 2日目\n",
    "\n",
    "2 日目のデータが手に入るので、1 日目のデータと合わせて再学習し、モデルを評価します。 1 日目のモデルより評価が高ければ 2 日目のモデルをデプロイします。これらを手動で動かすのは辛いので SageMaker Pipelines で動かすようセットアップしたあとパイプラインを実行します。また、モデルレジストリにモデルも登録します。\n",
    "\n",
    "\n",
    "\n",
    "### 準備\n",
    "まずは、データを入手し、3 分割する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, sagemaker, pandas as pd, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.steps import ProcessingStep,TrainingStep\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep, JsonGet\n",
    "from sagemaker.workflow.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データをダウンロード\n",
    "![ -e DKD2e_data_sets.zip ] && rm DKD2e_data_sets.zip\n",
    "!wget http://dataminingconsultant.com/DKD2e_data_sets.zip\n",
    "!unzip -o DKD2e_data_sets.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用するデータを確認\n",
    "df = pd.read_csv('./Data sets/churn.txt')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データを分割する際、離反データが偏らないように、離反したデータと離反しなかったデータを分けて分割する\n",
    "df_true = df[df['Churn?']=='True.'].reset_index()\n",
    "df_false = df[df['Churn?']=='False.'].reset_index()\n",
    "df_true = df_true.drop(['index'],axis=1)\n",
    "df_false = df_false.drop(['index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割前にシャッフルする\n",
    "df_true_shuffle = df_true.sample(frac=1, random_state=42)\n",
    "df_false_shuffle = df_false.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3分割する\n",
    "split_num = 3\n",
    "split_df_list = []\n",
    "for i in range(split_num):\n",
    "    idx_min_true,idx_max_true = i*len(df_true)//split_num,(i+1)*len(df_true)//split_num\n",
    "    idx_min_false,idx_max_false = i*len(df_false)//split_num,(i+1)*len(df_false)//split_num\n",
    "    tmp_df = pd.concat([df_true[idx_min_true:idx_max_true],df_false[idx_min_false:idx_max_false]],axis=0)\n",
    "    split_df_list.append(tmp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割ファイルをローカルに出力する\n",
    "shots_dir = './rawdata/'\n",
    "os.makedirs(f'{shots_dir}/', exist_ok=True)\n",
    "local_csvfile_list = []\n",
    "for i,split_df in enumerate(split_df_list):\n",
    "    file_name = f'{shots_dir}day_{str(i+1)}.csv'\n",
    "    split_df.to_csv(file_name,index=False)\n",
    "    local_csvfile_list.append(file_name)\n",
    "print(*local_csvfile_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一日目のデータで前処理、学習、評価、デプロイ、予測を手作業で\n",
    "### 前処理\n",
    "前処理は[こちら](https://github.com/aws-samples/amazon-sagemaker-examples-jp/blob/master/xgboost_customer_churn/xgboost_customer_churn.ipynb)と同じことを SageMaker Processing で行う。コンテナは scikit-learn のビルトインコンテナを利用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processor 定義\n",
    "ROLE = get_execution_role()\n",
    "HANDSON_NAME = 'sagemaker-pipelines-sample'\n",
    "PRE_PROCESS_JOBNAME = f'{HANDSON_NAME}-pre-processing'\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    base_job_name = PRE_PROCESS_JOBNAME,\n",
    "    framework_version='0.23-1',\n",
    "    role=ROLE,\n",
    "    instance_type='ml.m5.xlarge',instance_count=1\n",
    ")\n",
    "\n",
    "BUCKET = sagemaker.session.Session().default_bucket()\n",
    "RAWDATA_PREFIX = shots_dir.replace('./','').replace('/','')\n",
    "RAWDATA_S3_URI = f's3://{BUCKET}/{HANDSON_NAME}/{RAWDATA_PREFIX}'\n",
    "\n",
    "# input 定義\n",
    "RAWCSV_S3_URI = sagemaker.s3.S3Uploader.upload(local_csvfile_list[0],RAWDATA_S3_URI)\n",
    "PRE_PROCESSING_RAW_INPUT_DIR = '/opt/ml/processing/input/raw'\n",
    "\n",
    "# output 定義\n",
    "PRE_PROCESSING_TRAIN_OUTPUT_DIR = '/opt/ml/processing/output/train'\n",
    "PRE_PROCESSING_VALID_OUTPUT_DIR = '/opt/ml/processing/output/valid'\n",
    "PRE_PROCESSING_TEST_OUTPUT_DIR = '/opt/ml/processing/output/test'\n",
    "\n",
    "sklearn_processor.run(code='./preprocess/preprocess.py',\n",
    "                      # ProcessingInput は指定したものを全て S3 から processing インスタンスにコピーされる。 Destination でコピー先を指定できる。\n",
    "                      inputs=[\n",
    "                          ProcessingInput( \n",
    "                              source=RAWCSV_S3_URI,\n",
    "                              destination=PRE_PROCESSING_RAW_INPUT_DIR\n",
    "                          ),\n",
    "                      ],\n",
    "                      # processing インスタンスの source にあるものを全て S3 に格納する。(processing インスタンス側でこのディレクトリは自動で作成される)\n",
    "                      outputs=[\n",
    "                          ProcessingOutput(\n",
    "                              output_name = 'train',\n",
    "                              source=PRE_PROCESSING_TRAIN_OUTPUT_DIR,\n",
    "                          ),\n",
    "                          ProcessingOutput(\n",
    "                              output_name = 'valid',\n",
    "                              source=PRE_PROCESSING_VALID_OUTPUT_DIR,\n",
    "                          ),\n",
    "                          ProcessingOutput(\n",
    "                              output_name = 'test',\n",
    "                              source=PRE_PROCESSING_TEST_OUTPUT_DIR,\n",
    "                          )\n",
    "                      ],\n",
    "                      # processing インスタンスのどこに csv ファイルが配置されたか、どこにファイルを出力すればよいのか、を\n",
    "                      # コードに渡すための引数\n",
    "                      arguments=[\n",
    "                          '--raw-input-dir',PRE_PROCESSING_RAW_INPUT_DIR,\n",
    "                          '--train-output-dir',PRE_PROCESSING_TRAIN_OUTPUT_DIR,\n",
    "                          '--valid-output-dir',PRE_PROCESSING_VALID_OUTPUT_DIR,\n",
    "                          '--test-output-dir',PRE_PROCESSING_TEST_OUTPUT_DIR,\n",
    "                      ]\n",
    "                     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習\n",
    "xgboost を利用する。ハイパーパラメータは[こちら](https://github.com/aws-samples/amazon-sagemaker-examples-jp/blob/master/xgboost_customer_churn/xgboost_customer_churn.ipynb)と同じにして SageMaker Training で行う。   \n",
    "コンテナは xgboost のビルトインコンテナを利用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV_S3_URI = sklearn_processor.latest_job.describe()['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri'] + '/train.csv'\n",
    "VALID_CSV_S3_URI = sklearn_processor.latest_job.describe()['ProcessingOutputConfig']['Outputs'][1]['S3Output']['S3Uri'] + '/valid.csv'\n",
    "TEST_CSV_S3_URI = sklearn_processor.latest_job.describe()['ProcessingOutputConfig']['Outputs'][2]['S3Output']['S3Uri'] + '/test.csv'\n",
    "print(TRAIN_CSV_S3_URI)\n",
    "print(VALID_CSV_S3_URI)\n",
    "print(TEST_CSV_S3_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENT_TYPE='text/csv'\n",
    "TRAIN_S3_INPUT = TrainingInput(TRAIN_CSV_S3_URI, content_type=CONTENT_TYPE)\n",
    "VALID_S3_INPUT = TrainingInput(VALID_CSV_S3_URI, content_type=CONTENT_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_CONTAINER = sagemaker.image_uris.retrieve(\"xgboost\", sagemaker.session.Session().boto_region_name, \"1.2-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_JOBNAME = f'{HANDSON_NAME}-traing'\n",
    "MODEL_S3_URI = f's3://{BUCKET}/{TRAIN_JOBNAME}'\n",
    "HYPERPARAMETERS = {\n",
    "    \"max_depth\":\"5\",\n",
    "    \"eta\":\"0.2\",\n",
    "    \"gamma\":\"4\",\n",
    "    \"min_child_weight\":\"6\",\n",
    "    \"subsample\":\"0.8\",\n",
    "    \"objective\":\"binary:logistic\",\n",
    "    \"num_round\":\"100\"\n",
    "}\n",
    "xgb = Estimator(\n",
    "    XGB_CONTAINER,\n",
    "    ROLE,\n",
    "    base_job_name = TRAIN_JOBNAME,\n",
    "    hyperparameters=HYPERPARAMETERS,\n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path = MODEL_S3_URI\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit({'train': TRAIN_S3_INPUT, 'validation': VALID_S3_INPUT})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの評価\n",
    "* AUC で行う\n",
    "* SageMaker Processing を利用する\n",
    "* xgboost のビルトインコンテナを利用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_DATA_S3_URI = xgb.model_data\n",
    "POST_PROCESS_JOBNAME = f'{HANDSON_NAME}-post-processing'\n",
    "POST_PROCESSING_OUTPUT_S3_URI = f's3://{BUCKET}/{POST_PROCESS_JOBNAME}/post_processing_output'\n",
    "POST_PROCESSING_MODEL_DIR = '/opt/ml/processing/model'\n",
    "POST_PROCESSING_INPUT_DIR = '/opt/ml/processing/input'\n",
    "POST_PROCESSING_OUTPUT_DIR = '/opt/ml/processing/output'\n",
    "\n",
    "eval_processor = ScriptProcessor(\n",
    "    base_job_name = POST_PROCESS_JOBNAME,\n",
    "    image_uri=XGB_CONTAINER,\n",
    "    command=[\"python3\"],\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    "    role=ROLE,\n",
    ")\n",
    "eval_processor.run(\n",
    "    code = './postprocess/postprocess.py',\n",
    "    inputs=[\n",
    "        ProcessingInput( \n",
    "            source=TEST_CSV_S3_URI,\n",
    "            destination=POST_PROCESSING_INPUT_DIR\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=MODEL_DATA_S3_URI,\n",
    "            destination=POST_PROCESSING_MODEL_DIR\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            source=POST_PROCESSING_OUTPUT_DIR,\n",
    "            destination=POST_PROCESSING_OUTPUT_S3_URI\n",
    "        )\n",
    "    ],\n",
    "    arguments=[\n",
    "        '--input-dir',POST_PROCESSING_INPUT_DIR,\n",
    "        '--output-dir',POST_PROCESSING_OUTPUT_DIR,\n",
    "        '--model-dir',POST_PROCESSING_MODEL_DIR,\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 日目はパイプラインを作成する\n",
    "1 日目とほぼ同じことをするのでパイプラインで自動化する。\n",
    "追加する処理として、1 日目のデータで学習したモデルと 2 日目のデータを追加して学習したモデルで精度を比較し、よかったほうをデプロイする、という処理もパイプラインで実現する。\n",
    "今まで大文字で書いていた変数が SageMaker に渡していたパラメータですので、それらを Pipelines で扱えるようパラメータインスタンスにします。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "RAWCSV_S3_URI = sagemaker.s3.S3Uploader.upload(local_csvfile_list[1],RAWDATA_S3_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_CSV_S3_URI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理ステップ定義\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    base_job_name = PRE_PROCESS_JOBNAME,\n",
    "    framework_version='0.23-1',\n",
    "    role=ROLE,\n",
    "    instance_type='ml.m5.xlarge',instance_count=1\n",
    ")\n",
    "\n",
    "rawcsv_s3_uri_param = ParameterString(name='RawCsvS3Uri',default_value=RAWCSV_S3_URI)\n",
    "\n",
    "\n",
    "LASTTIME_PRE_PROCESSING_TRAIN_DATA_INPUT_DIR = '/opt/ml/processing/input/train'\n",
    "LASTTIME_PRE_PROCESSING_VALID_DATA_INPUT_DIR = '/opt/ml/processing/input/valid'\n",
    "LASTTIME_PRE_PROCESSING_TEST_DATA_INPUT_DIR = '/opt/ml/processing/input/test'\n",
    "lasttime_pre_processing_train_data_s3_uri_param = ParameterString(name='lasttime-pre-processing-train-data-S3-URI',default_value=TRAIN_CSV_S3_URI)\n",
    "lasttime_pre_processing_valid_data_s3_uri_param = ParameterString(name='lasttime-pre-processing-valid-data-S3-URI',default_value=VALID_CSV_S3_URI)\n",
    "lasttime_pre_processing_test_data_s3_uri_param = ParameterString(name='lasttime-pre-processing-test-data-S3-URI',default_value=TEST_CSV_S3_URI)\n",
    "\n",
    "pre_process_step = ProcessingStep(\n",
    "    code='./preprocess/preprocess.py',\n",
    "    name=f'{PRE_PROCESS_JOBNAME}_step',\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=rawcsv_s3_uri_param,\n",
    "            destination=PRE_PROCESSING_RAW_INPUT_DIR\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=lasttime_pre_processing_train_data_s3_uri_param,\n",
    "            destination=LASTTIME_PRE_PROCESSING_TRAIN_DATA_INPUT_DIR\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=lasttime_pre_processing_valid_data_s3_uri_param,\n",
    "            destination=LASTTIME_PRE_PROCESSING_VALID_DATA_INPUT_DIR\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=lasttime_pre_processing_test_data_s3_uri_param,\n",
    "            destination=LASTTIME_PRE_PROCESSING_TEST_DATA_INPUT_DIR\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name = 'train',\n",
    "            source=PRE_PROCESSING_TRAIN_OUTPUT_DIR,\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name = 'valid',\n",
    "            source=PRE_PROCESSING_VALID_OUTPUT_DIR,\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name = 'test',\n",
    "            source=PRE_PROCESSING_TEST_OUTPUT_DIR,\n",
    "        )\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        '--raw-input-dir',PRE_PROCESSING_RAW_INPUT_DIR,\n",
    "        '--lasttime-train-input-dir',LASTTIME_PRE_PROCESSING_TRAIN_DATA_INPUT_DIR,\n",
    "        '--lasttime-valid-input-dir',LASTTIME_PRE_PROCESSING_VALID_DATA_INPUT_DIR,\n",
    "        '--lasttime-test-input-dir',LASTTIME_PRE_PROCESSING_TEST_DATA_INPUT_DIR,\n",
    "        '--train-output-dir',PRE_PROCESSING_TRAIN_OUTPUT_DIR,\n",
    "        '--valid-output-dir',PRE_PROCESSING_VALID_OUTPUT_DIR,\n",
    "        '--test-output-dir',PRE_PROCESSING_TEST_OUTPUT_DIR,   \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習ステップ定義\n",
    "xgb = Estimator(\n",
    "    XGB_CONTAINER,\n",
    "    ROLE,\n",
    "    base_job_name = TRAIN_JOBNAME,\n",
    "    hyperparameters=HYPERPARAMETERS,\n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path = MODEL_S3_URI\n",
    ")\n",
    "\n",
    "train_step = TrainingStep(\n",
    "    name=f\"{TRAIN_JOBNAME}_step\",\n",
    "    estimator=xgb,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=pre_process_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=CONTENT_TYPE\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=pre_process_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"valid\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=CONTENT_TYPE\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "thistime_train_eval_processor = ScriptProcessor(\n",
    "    base_job_name = f'{HANDSON_NAME}_thistime_train_eval',\n",
    "    image_uri=XGB_CONTAINER,\n",
    "    command=['python3'],\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    "    role=ROLE,\n",
    ")\n",
    "thistime_train_eval_report = PropertyFile(\n",
    "    name='thistime_train_evaluation_report',\n",
    "    output_name='thistime_train_evaluation',\n",
    "    path='evaluation.json'\n",
    ")\n",
    "\n",
    "thistime_train_eval_step = ProcessingStep(\n",
    "    code='./postprocess/postprocess.py',\n",
    "    name=f'{HANDSON_NAME}_thistime_train_eval_step',\n",
    "    processor=thistime_train_eval_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=POST_PROCESSING_MODEL_DIR\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=pre_process_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                'test'\n",
    "            ].S3Output.S3Uri,\n",
    "            destination=POST_PROCESSING_INPUT_DIR\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='thistime_train_evaluation',\n",
    "            source=POST_PROCESSING_OUTPUT_DIR\n",
    "        ),\n",
    "    ],\n",
    "    property_files=[thistime_train_eval_report],\n",
    "    job_arguments=[\n",
    "        '--input-dir',POST_PROCESSING_INPUT_DIR,\n",
    "        '--output-dir',POST_PROCESSING_OUTPUT_DIR,\n",
    "        '--model-dir',POST_PROCESSING_MODEL_DIR,        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lasttime_train_eval_processor = ScriptProcessor(\n",
    "    base_job_name = f'{HANDSON_NAME}_lasttime_train_eval',\n",
    "    image_uri=XGB_CONTAINER,\n",
    "    command=['python3'],\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    "    role=ROLE,\n",
    ")\n",
    "lasttime_train_eval_report = PropertyFile(\n",
    "    name='lasttime_train_evaluation_report',\n",
    "    output_name='lasttime_train_evaluation',\n",
    "    path='lasttime_train_evaluation.json'\n",
    ")\n",
    "lasttime_train_model_s3_uri_param = ParameterString(name='lasttime-train-model-S3-URI',default_value=MODEL_DATA_S3_URI)\n",
    "lasttime_train_eval_step = ProcessingStep(\n",
    "    code='./postprocess/postprocess.py',\n",
    "    name=f'{HANDSON_NAME}_lasttime_train_eval_step',\n",
    "    processor=lasttime_train_eval_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=lasttime_train_model_s3_uri_param,\n",
    "            destination=POST_PROCESSING_MODEL_DIR\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=pre_process_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                'test'\n",
    "            ].S3Output.S3Uri,\n",
    "            destination=POST_PROCESSING_INPUT_DIR\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='lasttime_train_evaluation',\n",
    "            source=POST_PROCESSING_OUTPUT_DIR\n",
    "        ),\n",
    "    ],\n",
    "    property_files=[lasttime_train_eval_report],\n",
    "    job_arguments=[\n",
    "        '--input-dir',POST_PROCESSING_INPUT_DIR,\n",
    "        '--output-dir',POST_PROCESSING_OUTPUT_DIR,\n",
    "        '--model-dir',POST_PROCESSING_MODEL_DIR,        \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    image_uri=XGB_CONTAINER,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sagemaker.session.Session(),\n",
    "    role=ROLE,\n",
    ")\n",
    "model_inputs = CreateModelInput(\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    # accelerator_type=\"ml.eia1.medium\",\n",
    ")\n",
    "create_model_step = CreateModelStep(\n",
    "    name=f\"{HANDSON_NAME}-create-model-step\",\n",
    "    model=model,\n",
    "    inputs=model_inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_gte = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step=thistime_train_eval_step,\n",
    "        property_file=thistime_train_eval_report,\n",
    "        json_path=\"regression_metrics.auc.value\",\n",
    "    ),\n",
    "    right=JsonGet(\n",
    "        step=lasttime_train_eval_step,\n",
    "        property_file=lasttime_train_eval_report,\n",
    "        json_path=\"regression_metrics.auc.value\",\n",
    "    ),\n",
    ")\n",
    "\n",
    "cond_step = ConditionStep(\n",
    "    name=f'{HANDSON_NAME}_condtion',\n",
    "    conditions=[cond_gte],\n",
    "    if_steps=[create_model_step],\n",
    "    else_steps=[], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name = f\"{HANDSON_NAME}-pipeline\"\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        lasttime_pre_processing_train_data_s3_uri_param,\n",
    "        lasttime_pre_processing_valid_data_s3_uri_param,\n",
    "        lasttime_pre_processing_test_data_s3_uri_param,\n",
    "        lasttime_train_model_s3_uri_param\n",
    "    ],\n",
    "    steps=[\n",
    "        pre_process_step,\n",
    "        train_step,\n",
    "        thistime_train_eval_step,\n",
    "        lasttime_train_eval_step,\n",
    "        create_model_step,\n",
    "        # cond_step\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=ROLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pre_process_step)\n",
    "print(train_step,)\n",
    "print(thistime_train_eval_step,)\n",
    "print(lasttime_train_eval_step,)\n",
    "print(create_model_step,)\n",
    "print(cond_step)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawcsv_s3_uri_param"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-northeast-1:102112518831:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
