{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 離反予測を用いた SageMaker Pipelines の ML パイプライン構築\n",
    "\n",
    "## シナリオ\n",
    "\n",
    "電話回線の離反データセット（回線ごとのデータと離反した/しなかったの結果が残る）を使って、\n",
    "SageMaker Pipelines を用いたML パイプラインを構築します。\n",
    "データの詳細については[こちら](https://github.com/aws-samples/amazon-sagemaker-examples-jp/blob/master/xgboost_customer_churn/xgboost_customer_churn.ipynb)に詳細があります。  \n",
    "\n",
    "3333 行の 元データを 1111 行ずつ 3 分割し、それぞれ 1 日目に入手するデータ、 2 日目に入手するデータ、 3 日目に入手するデータと仮定し、\n",
    "* 1 日目は今あるデータを SageMaker Processing, Training, hosting をそれぞれ手動で動かす。\n",
    "* 2 日目は 1 日目のデータに加えて、 2 日目に手に入ったデータも利用して学習し、 1 日目と 2 日目のモデルを比較して、2 日目のほうが精度がよければ 2 日目のモデルを hosting するのを、パイプラインを構築して実行する。\n",
    "* 3 日目は 1 日目と 2 日目のデータに加えて、3 日目に手に入ったデータも利用して学習し、2 日目と 3 日目のモデルを比較して、3 日目のほうが精度がよければ 3 日目のモデルを hosting するのを、2 日目に作成したパイプラインのパラメータだけを変更して実行する。\n",
    "* 4 日目は精度が下がる場合のテストとして、ノイズデータを加えて学習し、パイプラインで精度が落ちたときは モデルが更新 されないことを確認する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, json, sagemaker, pandas as pd, numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.processing import ProcessingInput, ProcessingOutput, ScriptProcessor\n",
    "from sagemaker.inputs import TrainingInput\n",
    "from sagemaker import get_execution_role\n",
    "from sagemaker.workflow.parameters import ParameterInteger, ParameterString\n",
    "from sagemaker.workflow.properties import PropertyFile\n",
    "from sagemaker.workflow.steps import ProcessingStep,TrainingStep\n",
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.inputs import CreateModelInput\n",
    "from sagemaker.workflow.steps import CreateModelStep\n",
    "from sagemaker.transformer import Transformer\n",
    "from sagemaker.workflow.conditions import ConditionGreaterThanOrEqualTo\n",
    "from sagemaker.workflow.condition_step import ConditionStep, JsonGet\n",
    "from sagemaker.workflow.pipeline import Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## データ準備   \n",
    "T/F の割合が変わらないように、3333 行のデータを 3 分割する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データをダウンロード\n",
    "![ -e DKD2e_data_sets.zip ] && rm DKD2e_data_sets.zip\n",
    "!wget http://dataminingconsultant.com/DKD2e_data_sets.zip\n",
    "!unzip -o DKD2e_data_sets.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 使用するデータを確認\n",
    "df = pd.read_csv('./Data sets/churn.txt')\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# データを分割する際、離反データが偏らないように、離反したデータと離反しなかったデータを分けて分割する\n",
    "df_true = df[df['Churn?']=='True.'].reset_index()\n",
    "df_false = df[df['Churn?']=='False.'].reset_index()\n",
    "df_true = df_true.drop(['index'],axis=1)\n",
    "df_false = df_false.drop(['index'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割前にシャッフルする\n",
    "df_true_shuffle = df_true.sample(frac=1, random_state=42)\n",
    "df_false_shuffle = df_false.sample(frac=1, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3分割する\n",
    "split_num = 3\n",
    "split_df_list = []\n",
    "for i in range(split_num):\n",
    "    idx_min_true,idx_max_true = i*len(df_true)//split_num,(i+1)*len(df_true)//split_num\n",
    "    idx_min_false,idx_max_false = i*len(df_false)//split_num,(i+1)*len(df_false)//split_num\n",
    "    tmp_df = pd.concat([df_true[idx_min_true:idx_max_true],df_false[idx_min_false:idx_max_false]],axis=0)\n",
    "    split_df_list.append(tmp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 分割ファイルをローカルに出力する\n",
    "RAWDATA_DIR = './raw_data/'\n",
    "os.makedirs(f'{RAWDATA_DIR}/', exist_ok=True)\n",
    "local_csvfile_list = []\n",
    "for i,split_df in enumerate(split_df_list):\n",
    "    file_name = f'{RAWDATA_DIR}day_{str(i+1)}.csv'\n",
    "    split_df.to_csv(file_name,index=False)\n",
    "    local_csvfile_list.append(file_name)\n",
    "print(*local_csvfile_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一日目のデータで前処理、学習、評価、デプロイを手作業で\n",
    "### 前処理\n",
    "前処理は[こちら](https://github.com/aws-samples/amazon-sagemaker-examples-jp/blob/master/xgboost_customer_churn/xgboost_customer_churn.ipynb)と同じことを SageMaker Processing で行う。コンテナは scikit-learn のビルトインコンテナを利用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processor 定義\n",
    "ROLE = get_execution_role()\n",
    "PIPELINE_NAME = 'PL-test'\n",
    "PRE_PROCESS_JOBNAME = f'{PIPELINE_NAME}-pre-process'\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    base_job_name = PRE_PROCESS_JOBNAME,\n",
    "    framework_version='0.23-1',\n",
    "    role=ROLE,\n",
    "    instance_type='ml.m5.xlarge',instance_count=1\n",
    ")\n",
    "\n",
    "BUCKET = sagemaker.session.Session().default_bucket()\n",
    "RAWDATA_SUB_PREFIX = RAWDATA_DIR.replace('./','').replace('/','')\n",
    "RAWDATA_S3_URI = f's3://{BUCKET}/{PIPELINE_NAME}-{RAWDATA_SUB_PREFIX}'\n",
    "\n",
    "# input 定義\n",
    "rawcsv_s3_uri = sagemaker.s3.S3Uploader.upload(local_csvfile_list[0],RAWDATA_S3_URI)\n",
    "PRE_PROCESS_RAW_DATA_INPUT_DIR = '/opt/ml/processing/input/raw_data'\n",
    "\n",
    "# output 定義\n",
    "PRE_PROCESS_TRAIN_OUTPUT_DIR = '/opt/ml/processing/output/train'\n",
    "PRE_PROCESS_VALID_OUTPUT_DIR = '/opt/ml/processing/output/valid'\n",
    "PRE_PROCESS_TEST_OUTPUT_DIR = '/opt/ml/processing/output/test'\n",
    "\n",
    "sklearn_processor.run(\n",
    "    code='./preprocess/preprocess.py',\n",
    "    # ProcessingInput は指定したものを全て S3 から processing インスタンスにコピーされる。 Destination でコピー先を指定できる。\n",
    "    inputs=[\n",
    "        ProcessingInput( \n",
    "            source=rawcsv_s3_uri,\n",
    "            destination=PRE_PROCESS_RAW_DATA_INPUT_DIR\n",
    "        ),\n",
    "    ],\n",
    "    # processing インスタンスの source にあるものを全て S3 に格納する。(processing インスタンス側でこのディレクトリは自動で作成される)\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name = 'train',\n",
    "            source=PRE_PROCESS_TRAIN_OUTPUT_DIR,\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name = 'valid',\n",
    "            source=PRE_PROCESS_VALID_OUTPUT_DIR,\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name = 'test',\n",
    "            source=PRE_PROCESS_TEST_OUTPUT_DIR,\n",
    "        )\n",
    "    ],\n",
    "    # processing インスタンスのどこに csv ファイルが配置されたか、どこにファイルを出力すればよいのか、を\n",
    "    # コードに渡すための引数\n",
    "    arguments=[\n",
    "        '--raw-data-input-dir',PRE_PROCESS_RAW_DATA_INPUT_DIR,\n",
    "        '--train-output-dir',PRE_PROCESS_TRAIN_OUTPUT_DIR,\n",
    "        '--valid-output-dir',PRE_PROCESS_VALID_OUTPUT_DIR,\n",
    "        '--test-output-dir',PRE_PROCESS_TEST_OUTPUT_DIR,\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 学習\n",
    "xgboost を利用する。ハイパーパラメータは[こちら](https://github.com/aws-samples/amazon-sagemaker-examples-jp/blob/master/xgboost_customer_churn/xgboost_customer_churn.ipynb)と同じにして SageMaker Training で行う。   \n",
    "コンテナは xgboost のビルトインコンテナを利用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_csv_s3_uri = sklearn_processor.latest_job.describe()['ProcessingOutputConfig']['Outputs'][0]['S3Output']['S3Uri'] + '/train.csv'\n",
    "valid_csv_s3_uri = sklearn_processor.latest_job.describe()['ProcessingOutputConfig']['Outputs'][1]['S3Output']['S3Uri'] + '/valid.csv'\n",
    "test_csv_s3_uri = sklearn_processor.latest_job.describe()['ProcessingOutputConfig']['Outputs'][2]['S3Output']['S3Uri'] + '/test.csv'\n",
    "print(train_csv_s3_uri)\n",
    "print(valid_csv_s3_uri)\n",
    "print(test_csv_s3_uri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CONTENT_TYPE='text/csv'\n",
    "train_s3_input = TrainingInput(train_csv_s3_uri, content_type=CONTENT_TYPE)\n",
    "valid_s3_input = TrainingInput(valid_csv_s3_uri, content_type=CONTENT_TYPE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "XGB_CONTAINER_URI = sagemaker.image_uris.retrieve(\"xgboost\", sagemaker.session.Session().boto_region_name, \"1.2-1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_JOBNAME = f'{PIPELINE_NAME}-train'\n",
    "MODEL_S3_URI = f's3://{BUCKET}/{TRAIN_JOBNAME}'\n",
    "HYPERPARAMETERS = {\n",
    "    \"max_depth\":\"5\",\n",
    "    \"eta\":\"0.2\",\n",
    "    \"gamma\":\"4\",\n",
    "    \"min_child_weight\":\"6\",\n",
    "    \"subsample\":\"0.8\",\n",
    "    \"objective\":\"binary:logistic\",\n",
    "    \"num_round\":\"100\"\n",
    "}\n",
    "xgb = Estimator(\n",
    "    XGB_CONTAINER_URI,\n",
    "    ROLE,\n",
    "    base_job_name = TRAIN_JOBNAME,\n",
    "    hyperparameters=HYPERPARAMETERS,\n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path = MODEL_S3_URI\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xgb.fit({'train': train_s3_input, 'validation': valid_s3_input})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### モデルの評価\n",
    "今後に備えて、モデルを評価するスクリプトを作成し、動かしておく。\n",
    "* 評価は AUC で行う\n",
    "* SageMaker Processing を利用する\n",
    "* xgboost のビルトインコンテナを利用する"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_s3_uri = xgb.model_data\n",
    "POST_PROCESS_JOBNAME = f'{PIPELINE_NAME}-post-process'\n",
    "POST_PROCESS_INPUT_MODEL_DIR = '/opt/ml/processing/input/model'\n",
    "POST_PROCESS_INPUT_DATA_DIR = '/opt/ml/processing/input/data'\n",
    "POST_PROCESS_OUTPUT_DIR = '/opt/ml/processing/output'\n",
    "\n",
    "eval_processor = ScriptProcessor(\n",
    "    base_job_name = POST_PROCESS_JOBNAME,\n",
    "    image_uri=XGB_CONTAINER_URI,\n",
    "    command=[\"python3\"],\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    "    role=ROLE,\n",
    ")\n",
    "eval_processor.run(\n",
    "    code = './postprocess/postprocess.py',\n",
    "    inputs=[\n",
    "        ProcessingInput( \n",
    "            source=test_csv_s3_uri,\n",
    "            destination=POST_PROCESS_INPUT_DATA_DIR\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=model_s3_uri,\n",
    "            destination=POST_PROCESS_INPUT_MODEL_DIR\n",
    "        )\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            source=POST_PROCESS_OUTPUT_DIR,\n",
    "        )\n",
    "    ],\n",
    "    arguments=[\n",
    "        '--input-model-dir',POST_PROCESS_INPUT_MODEL_DIR,\n",
    "        '--input-data-dir',POST_PROCESS_INPUT_DATA_DIR,\n",
    "        '--output-dir',POST_PROCESS_OUTPUT_DIR,\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 日目はパイプラインを作成する\n",
    "新しくデータが入ってくるので、追加データも併せてモデルを学習しなおして精度を確認し、精度が上がっていたらモデルを交換する。\n",
    "1 日目とほぼ同じことをやるので、パイプラインを作成して省力化する。\n",
    "前日に追加する処理として、1 日目のデータで学習したモデルと 2 日目のデータを追加して学習したモデルで精度を比較し、精度が上がっていたらモデルを差し替える、オペレーションを追加する。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper 関数\n",
    "# pipeline で利用する名前は camel case を使うのが一般的なので、区切り文字を削除し、頭を大文字にする関数を準備\n",
    "def to_camel(s_v:str,s_s:str)->str:\n",
    "    '''\n",
    "    s_v: camel_case に変えたい文字\n",
    "    s_s: 区切り文字\n",
    "    '''\n",
    "    return ''.join(word.title() for word in s_v.split(s_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rawcsv_s3_uri = sagemaker.s3.S3Uploader.upload(local_csvfile_list[1],RAWDATA_S3_URI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 前処理ステップ定義\n",
    "\n",
    "sklearn_processor = SKLearnProcessor(\n",
    "    base_job_name = PRE_PROCESS_JOBNAME,\n",
    "    framework_version='0.23-1',\n",
    "    role=ROLE,\n",
    "    instance_type='ml.m5.xlarge',instance_count=1\n",
    ")\n",
    "\n",
    "rawcsv_s3_uri_param = ParameterString(name='RawCsvS3Uri',default_value=rawcsv_s3_uri)\n",
    "\n",
    "PRE_PROCESSED_TRAIN_DATA_INPUT_DIR = '/opt/ml/processing/input/train'\n",
    "PRE_PROCESSED_VALID_DATA_INPUT_DIR = '/opt/ml/processing/input/valid'\n",
    "PRE_PROCESSED_TEST_DATA_INPUT_DIR = '/opt/ml/processing/input/test'\n",
    "\n",
    "# Pipeline 実行時に渡すパラメータ設定\n",
    "# 名前はキャメルケース\n",
    "pre_processed_train_data_s3_uri_param = ParameterString(name='PreProcessedTrainDataS3UriParam',default_value=train_csv_s3_uri)\n",
    "pre_processed_valid_data_s3_uri_param = ParameterString(name='PreProcessedValidDataS3UriParam',default_value=valid_csv_s3_uri)\n",
    "pre_processed_test_data_s3_uri_param = ParameterString(name='PreProcessedTestDataS3UriParam',default_value=test_csv_s3_uri)\n",
    "\n",
    "pre_process_step = ProcessingStep(\n",
    "    code='./preprocess/preprocess.py',\n",
    "    name=f'{to_camel(PRE_PROCESS_JOBNAME,\"-\")}Step',\n",
    "    processor=sklearn_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=rawcsv_s3_uri_param,\n",
    "            destination=PRE_PROCESS_RAW_DATA_INPUT_DIR\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=pre_processed_train_data_s3_uri_param,\n",
    "            destination=PRE_PROCESSED_TRAIN_DATA_INPUT_DIR\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=pre_processed_valid_data_s3_uri_param,\n",
    "            destination=PRE_PROCESSED_VALID_DATA_INPUT_DIR\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=pre_processed_test_data_s3_uri_param,\n",
    "            destination=PRE_PROCESSED_TEST_DATA_INPUT_DIR\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name = 'train',\n",
    "            source=PRE_PROCESS_TRAIN_OUTPUT_DIR,\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name = 'valid',\n",
    "            source=PRE_PROCESS_VALID_OUTPUT_DIR,\n",
    "        ),\n",
    "        ProcessingOutput(\n",
    "            output_name = 'test',\n",
    "            source=PRE_PROCESS_TEST_OUTPUT_DIR,\n",
    "        )\n",
    "    ],\n",
    "    job_arguments=[\n",
    "        '--raw-data-input-dir',PRE_PROCESS_RAW_DATA_INPUT_DIR,\n",
    "        '--pre-processed-train-data-input-dir',PRE_PROCESSED_TRAIN_DATA_INPUT_DIR,\n",
    "        '--pre-processed-valid-data-input-dir',PRE_PROCESSED_VALID_DATA_INPUT_DIR,\n",
    "        '--pre-processed-test-data-input-dir',PRE_PROCESSED_TEST_DATA_INPUT_DIR,\n",
    "        '--train-output-dir',PRE_PROCESS_TRAIN_OUTPUT_DIR,\n",
    "        '--valid-output-dir',PRE_PROCESS_VALID_OUTPUT_DIR,\n",
    "        '--test-output-dir',PRE_PROCESS_TEST_OUTPUT_DIR,   \n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学習ステップ定義\n",
    "xgb = Estimator(\n",
    "    XGB_CONTAINER_URI,\n",
    "    ROLE,\n",
    "    base_job_name = TRAIN_JOBNAME,\n",
    "    hyperparameters=HYPERPARAMETERS,\n",
    "    instance_count=1, \n",
    "    instance_type='ml.m5.xlarge',\n",
    "    output_path = MODEL_S3_URI\n",
    ")\n",
    "\n",
    "train_step = TrainingStep(\n",
    "    name=f'{to_camel(TRAIN_JOBNAME,\"-\")}Step',\n",
    "    estimator=xgb,\n",
    "    inputs={\n",
    "        \"train\": TrainingInput(\n",
    "            s3_data=pre_process_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"train\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=CONTENT_TYPE\n",
    "        ),\n",
    "        \"validation\": TrainingInput(\n",
    "            s3_data=pre_process_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                \"valid\"\n",
    "            ].S3Output.S3Uri,\n",
    "            content_type=CONTENT_TYPE\n",
    "        )\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LASTTIME_EVALUATION_FILE = 'thistime_evaluation.json'\n",
    "thistime_train_eval_processor = ScriptProcessor(\n",
    "    base_job_name = f'{POST_PROCESS_JOBNAME}-thistime-train-eval',\n",
    "    image_uri=XGB_CONTAINER_URI,\n",
    "    command=['python3'],\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    "    role=ROLE,\n",
    ")\n",
    "thistime_train_eval_report = PropertyFile(\n",
    "    name='ThistimeTrainEvaluationReport',\n",
    "    output_name='ThistimeTrainEvaluation',\n",
    "    path=LASTTIME_EVALUATION_FILE\n",
    ")\n",
    "\n",
    "thistime_train_eval_step = ProcessingStep(\n",
    "    code='./postprocess/postprocess.py',\n",
    "    name=f'{to_camel(POST_PROCESS_JOBNAME,\"-\")}ThistimeTrainEvalStep',\n",
    "    processor=thistime_train_eval_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=pre_process_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                'test'\n",
    "            ].S3Output.S3Uri,\n",
    "            destination=POST_PROCESS_INPUT_DATA_DIR\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "            destination=POST_PROCESS_INPUT_MODEL_DIR\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='ThistimeTrainEvaluation',\n",
    "            source=POST_PROCESS_OUTPUT_DIR\n",
    "        ),\n",
    "    ],\n",
    "    property_files=[thistime_train_eval_report],\n",
    "    job_arguments=[\n",
    "        '--input-data-dir',POST_PROCESS_INPUT_DATA_DIR,\n",
    "        '--input-model-dir',POST_PROCESS_INPUT_MODEL_DIR,\n",
    "        '--output-dir',POST_PROCESS_OUTPUT_DIR,\n",
    "        '--output-file',LASTTIME_EVALUATION_FILE\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "LASTTIME_EVALUATION_FILE = 'lasttime_evaluation.json'\n",
    "lasttime_train_eval_processor = ScriptProcessor(\n",
    "    base_job_name = f'{POST_PROCESS_JOBNAME}-lasttime-train-eval',\n",
    "    image_uri=XGB_CONTAINER_URI,\n",
    "    command=['python3'],\n",
    "    instance_type='ml.m5.xlarge',\n",
    "    instance_count=1,\n",
    "    role=ROLE,\n",
    ")\n",
    "lasttime_train_eval_report = PropertyFile(\n",
    "    name='LasttimeTrainEvaluationReport',\n",
    "    output_name='LasttimeTrainEvaluation',\n",
    "    path=LASTTIME_EVALUATION_FILE\n",
    ")\n",
    "lasttime_train_model_s3_uri_param = ParameterString(\n",
    "    name='lasttime-train-model-S3-URI',\n",
    "    default_value=model_s3_uri\n",
    ")\n",
    "lasttime_train_eval_step = ProcessingStep(\n",
    "    code='./postprocess/postprocess.py',\n",
    "    name=f'{to_camel(POST_PROCESS_JOBNAME,\"-\")}LasttimeTrainEvalStep',\n",
    "    processor=lasttime_train_eval_processor,\n",
    "    inputs=[\n",
    "        ProcessingInput(\n",
    "            source=pre_process_step.properties.ProcessingOutputConfig.Outputs[\n",
    "                'test'\n",
    "            ].S3Output.S3Uri,\n",
    "            destination=POST_PROCESS_INPUT_DATA_DIR\n",
    "        ),\n",
    "        ProcessingInput(\n",
    "            source=lasttime_train_model_s3_uri_param,\n",
    "            destination=POST_PROCESS_INPUT_MODEL_DIR\n",
    "        ),\n",
    "    ],\n",
    "    outputs=[\n",
    "        ProcessingOutput(\n",
    "            output_name='LasttimeTrainEvaluation',\n",
    "            source=POST_PROCESS_OUTPUT_DIR\n",
    "        ),\n",
    "    ],\n",
    "    property_files=[lasttime_train_eval_report],\n",
    "    job_arguments=[\n",
    "        '--input-data-dir',POST_PROCESS_INPUT_DATA_DIR,\n",
    "        '--input-model-dir',POST_PROCESS_INPUT_MODEL_DIR,\n",
    "        '--output-dir',POST_PROCESS_OUTPUT_DIR,\n",
    "        '--output-file',LASTTIME_EVALUATION_FILE\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(\n",
    "    image_uri=XGB_CONTAINER_URI,\n",
    "    model_data=train_step.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sagemaker.session.Session(),\n",
    "    role=ROLE,\n",
    ")\n",
    "model_inputs = CreateModelInput(\n",
    "    instance_type=\"ml.m5.large\",\n",
    ")\n",
    "create_model_step = CreateModelStep(\n",
    "    name=f'{to_camel(PIPELINE_NAME,\"-\")}CreateModelStep',\n",
    "    model=model,\n",
    "    inputs=model_inputs,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cond_gte = ConditionGreaterThanOrEqualTo(\n",
    "    left=JsonGet(\n",
    "        step=thistime_train_eval_step,\n",
    "        property_file=thistime_train_eval_report,\n",
    "        json_path=\"classification_metrics.auc.value\",\n",
    "    ),\n",
    "    right=JsonGet(\n",
    "        step=lasttime_train_eval_step,\n",
    "        property_file=lasttime_train_eval_report,\n",
    "        json_path=\"classification_metrics.auc.value\",\n",
    "    ),\n",
    "#     right = 0.9\n",
    ")\n",
    "\n",
    "cond_step = ConditionStep(\n",
    "    name=f'{to_camel(PIPELINE_NAME,\"-\")}ConditionStep',\n",
    "    conditions=[cond_gte],\n",
    "    if_steps=[create_model_step],\n",
    "    else_steps=[], \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(\n",
    "    name=to_camel(PIPELINE_NAME,\"-\"),\n",
    "    parameters=[\n",
    "        rawcsv_s3_uri_param,\n",
    "        pre_processed_train_data_s3_uri_param,\n",
    "        pre_processed_valid_data_s3_uri_param,\n",
    "        pre_processed_test_data_s3_uri_param,\n",
    "        lasttime_train_model_s3_uri_param\n",
    "    ],\n",
    "    steps=[\n",
    "        pre_process_step,\n",
    "        train_step,\n",
    "        thistime_train_eval_step,\n",
    "        lasttime_train_eval_step,\n",
    "        cond_step,\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "definition = json.loads(pipeline.definition())\n",
    "definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=ROLE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution.wait()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 後片付け"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:ap-northeast-1:102112518831:image/datascience-1.0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
